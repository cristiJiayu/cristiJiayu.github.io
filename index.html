<!DOCTYPE HTML>
<html lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Jiayu Li's Homepage</title>
  <meta name="author" content="Jiayu Li">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <script src="js/jquery.min.js"></script>
  <script src="js/jquery.scrollzer.min.js"></script>
  <script src="js/jquery.scrolly.min.js"></script>
  <script src="js/skel.min.js"></script>
  <script src="js/skel-layers.min.js"></script>
  <script src="js/carousel.js"></script>
  <script type="text/javascript">
    function toggle_vis(id) {
      var e = document.getElementById(id);
      if (e.style.display == 'none')
        e.style.display = 'inline';
      else
        e.style.display = 'none';
    }
  </script>
  <!-- <script src="js/init.js"></script> -->
  <!--  <link rel="stylesheet" type="text/css" href="css/stylesheet.css"> -->
  <!--   <link rel="icon" type="image/png" href="images/xx.jpg"> -->
  <style type="text/css">
    .carousel {
      -webkit-transform: translate3d(0, 0, 0);
      background: rgba(0, 0, 0, 0.85);
      position: fixed;
      right: 0;
      bottom: 0;
      min-width: 100%;
      min-height: 100%;
      width: auto;
      height: auto;
      display: none;
      z-index: 1;
    }

    .img-center-carousel {
      position: absolute;
      top: 0;
      left: 0;
      right: 0;
      bottom: 0;
      padding: 0;
      margin: auto;
      width: 60%;
      height: auto;
    }

    .carousel-close {
      position: absolute;
      top: 25px;
      right: 100px;
      padding: 0;
      margin: auto;
      width: 50px;
      height: auto;
    }

    .research {
      margin-bottom: 30px;
    }

    .research h4 {
      float: left;
    }

    .research div {
      text-align: end;
      font-size: 0.9em;
    }

    .thumbnail {
      width: 100%;
      float: left;

    }

    .thumbnail-right {
      margin-left: 35%;
    }

    #exp li {
      margin-bottom: 50px;
    }

    .school-logo {
      width: 8%;
      float: left;
    }

    .school-text {
      margin-left: 10%;
      width: 60%;
    }
  </style>
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="stylesheet" href="https://cdn.staticfile.org/font-awesome/4.7.0/css/font-awesome.css">
</head>

<body>
  <!-- here for introduction -->
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Jiayu Li</name>
              </p>
              <p>Hi there! I’m a Ph.D. student of <a href="https://data.syr.edu/">Data Lab</a> at <a href="https://www.syracuse.edu/">Syracuse University</a>, advised by <a href="https://reza.zafarani.net/">Prof. Reza Zafarani</a>. I completed my master degree in Computer Science at <a href="https://www.syracuse.edu/">Syracuse University</a>. 
              </p>
<!--               <p style="text-align:justify">
                  <mark>
                    I am currently looking for 2023 summer internship related to research scientist or machine learning engineer in industry, please feel free to contact me if you are interested.
                  </mark>
              </p> -->
              <p style="text-align:center">
                <a href="mailto:jli221@data.syr.edu"><i class="fa fa-envelope"></i>&nbsp Email</a>&nbsp/&nbsp
                <a href="https://www.linkedin.com/in/jiayu-li-951641111/"><i class="fa fa-linkedin"></i>&nbsp LinkedIn</a>&nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=2Z_k5MUAAAAJ&hl=en"><i class="fa fa-graduation-cap"></i>&nbsp Google Scholar</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/jiayu_picture1.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/jiayu_picture1.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody>
  </table>

  <!-- here for research -->
  <table style="width:100%;border:0px;border-spacing:-2px;border-collapse:separate;margin-right:auto;margin-left:auto;margin-top:-50px"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p style="text-align:left;">
                My research mainly lies in the following fields of
                <ul>
                  <li> Machine Learning and Generative Graph Models with Reinforcement Learning; </li>
                  <li> Graph Representation and Sparsification on Large-scale Graphs; </li>
                  <li> Deep Neural Networks (DNNs) Compression and Pruning; </li>
                  <li> Natural Language Processing and Fake News Detection.</li>
                </ul>
              </p>
              <p style="text-align:justify">
                Much of my research now is related to <b>efficient machine learning models</b> in <b>graph mining</b>, <b>computer vision</b> and <b>natural language processing</b>. For example, based on graph neural networks (GNNs), graph signal processing (GSP) and graph spectral theory, I propose several graph sparsifiers to generate sparsified graphs, which can be used to construct better spectral-based filters for spectral-based GNNs, leading to the improvement of perforamnce of GNNs and the acceleration of both training and inference of GNNs. Meanwhile, in terms of pruning weights in Deep Neural Networks (DNNs), the inference of DNNs can accelerated in computer vision tasks without loss of performance. For an up-to-date publication list, please see my <a href="https://scholar.google.com/citations?user=2Z_k5MUAAAAJ&hl=en">Google Scholar</a>.
              </p>
            </td>
          </tr>
        </tbody></table>


  <!-- here for news -->
  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;margin-top:-10px"><tbody>
         <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>News</heading>
                  <p>
                  <ul>
                    <li> 02/2023: Our paper semi-supervised graph ultra-sparsifiers </a> is accepted at <b> ICASSP 2023 </b> </li>
                    <li> 12/2022: Our paper on <a href="https://www.researchgate.net/profile/Shengmin-Jin/publication/364089704_A_Spectral_Measure_for_Network_Robustness_Assessment_Design_and_Evolution/links/6338d72d769781354eaecee3/A-Spectral-Measure-for-Network-Robustness-Assessment-Design-and-Evolution.pdf"> spectral measure for network robustness</a> is accepted at <b> ICKG 2022 </b> </li>
                    <li> 08/2022: Our paper on <a href="https://dl.acm.org/doi/pdf/10.1145/3534678.3539433?casa_token=PO2Pr9m4FV0AAAAA:90IEhLc3rmUGlgofQE-GM7kgNi1dY9V8igdsAdqjfrS7s1NRwS4kGUZbEEsGroQW7qPVzaKKQsJ4GQ"> spectral representation of networks </a> is accepted at <b> KDD 2022 </b>  </li>
                    <li> 05/2022: Our paper <a href="https://reza.zafarani.net/publications/files/AdverSparse.pdf"> an adversarial attack framework for spatial-temporal GNNs </a> is accepted at <b> ICASSP 2022 </b>  </li>
                    <li> 11/2021: Our paper <a href="https://link.springer.com/article/10.1007/s41060-021-00288-8"> graph sparsification with graph convolutional networks </a> is accepted at <b> JDSA </b> </li>
                    <li> 01/2021: Our paper <a href="https://link.springer.com/chapter/10.1007/978-3-030-47426-3_22"> a graph sparsifier based on graph convolutional networks </a> is accepted at <b> PAKDD 2020 </b> </li> 

                     <li> <a href="javascript:toggle_vis('news')">More news</a> </li>
                    <div id="news" style="display:none">
                      <li> 04/2019: Our paper <a href="https://dl.acm.org/doi/pdf/10.1145/3297858.3304076"> an algorithm-hardware co-design framework of DNNs </a> is accepted at <b> ASPLOS 2019 </b>  </li>
                      <li> 11/2018: Our paper on <a href="https://ojs.aaai.org/index.php/AAAI/article/view/4475"> universal approximation property and equivalence </a> is accepted at <b> AAAI 2019 </b>  </li>
                      <li> 04/2018: Our paper <a href="https://dl.acm.org/doi/abs/10.1145/3194554.3194625"> structured weight matrices-based hardware accelerators in DNNs </a> is accepted at <b> GLSVLS 2018</b> </li>
                    </div>
                  </ul>
                  </p>
                </td>
              </tr>
         </tbody>
  </table>

  <!-- here for experience -->
  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;margin-top: -10px"><tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Experiences</heading>
                  <p>
                  <ul id="exp" style="list-style:none;">
                    <li> <img class="school-logo" src="images/Syracuse_logo.png">
                      <div class="school-text">Sep 2018 - Present, Syracuse University, <br />Research Assistant
                        <br /> Advisor: Prof. Reza Zafarani
                      </div>
                    </li>
                    <li> <img class="school-logo" src="images/Syracuse_logo.png">
                      <div class="school-text">Sep 2017 - Dec 2022, Syracuse University, <br />Graduate Teaching Assistant
                        <br/> Teaching Courses: Operating System, Randomized Algorithm and Artificial Neural Networks
                      </div>
                    </li>
                  </ul>
                  </p>
                </td>
              </tr>
          </tbody>
  </table>


  <!-- here for selected publications -->
  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20" margin-top="-20">
            <tr>
              <td style="padding:15px;width:35%;vertical-align:middle">
                <heading>Selected Publications</heading> <br>
                <a href="https://scholar.google.com/citations?user=2Z_k5MUAAAAJ&hl=en"><i class="fa fa-graduation-cap"></i>&nbsp Google Scholar for all publications </a>
              </td>
            </tr>
            <!--publication-->
            <tr>
               <td width="32%">
                <img class="thumbnail" src="images/icassp2023.jpg" data-id="hbar-carousel8"></img>
                <div id="hbar-carousel8" class="carousel">
                  <img class="img-center-carousel" src="images/icassp2023.jpg" alt="">
                  <a href="javascript:void(0)"><img src="images/icassp2023.jpg" alt="" data-carousel-id="hbar-carousel8" class="carousel-close"></a>
                </div>
              </td>

              <td style="padding:15px;width:80%;vertical-align:middle">
                <a href="">
                  <papertitle>Semi-Supervised Graph Ultra-Sparsifiers using Reweighted <i>L<sub>1</sub></i> Optimization</papertitle>
                </a>
                <br><b>Jiayu Li</b>, Tianyun Zhang, Shengmin Jin, Reza Zafarani<br>
                <em>Will appear in the IEEE International Conference on Acoustics, Speech and Signal Processing (<b>ICASSP</b>), 2023. <br> </em>
                [<a href="">paper</a>]
                <!-- [<a href="">paper</a>] [<a href="">code</a>] -->
                <br>
                <a href="javascript:toggle_vis('ICASSP2023')">More </a> 
                    <div id="ICASSP2023" style="display:none">
                      <p style="text-align:justify">
                        <ul>
                          <li></li>
                        </ul>
                      </p>
                    </div>
                </td>
            </tr>

            <!--publication-->
            <tr>
               <td width="32%">
                <img class="thumbnail" src="images/ickg2022.jpg" data-id="hbar-carousel"></img>
                <div id="hbar-carousel" class="carousel">
                  <img class="img-center-carousel" src="images/ickg2022.jpg" alt="">
                  <a href="javascript:void(0)"><img src="images/ickg2022.jpg" alt="" data-carousel-id="hbar-carousel" class="carousel-close"></a>
                </div>
              </td>

              <td style="padding:15px;width:80%;vertical-align:middle">
                <a href="https://www.researchgate.net/profile/Shengmin-Jin/publication/364089704_A_Spectral_Measure_for_Network_Robustness_Assessment_Design_and_Evolution/links/6338d72d769781354eaecee3/A-Spectral-Measure-for-Network-Robustness-Assessment-Design-and-Evolution.pdf">
                  <papertitle>A Spectral Measure for Network Robustness: Assessment, Design, and Evolution</papertitle>
                </a>
                <br>
                Shengmin Jin, Rui Ma, <b>Jiayu Li</b>, Sara Eftekharnejad, Reza Zafarani<br>
                <em>IEEE International Conference on Knowledge Graph (<b>ICKG</b>), 2022. <br> </em>
                [<a href="https://www.researchgate.net/profile/Shengmin-Jin/publication/364089704_A_Spectral_Measure_for_Network_Robustness_Assessment_Design_and_Evolution/links/6338d72d769781354eaecee3/A-Spectral-Measure-for-Network-Robustness-Assessment-Design-and-Evolution.pdf">paper</a>]
                <!-- [<a href="">paper</a>] [<a href="">code</a>] -->
                <br>
                <a href="javascript:toggle_vis('ICKG')">More </a> 
                    <div id="ICKG" style="display:none">
                      <p style="text-align:justify">A spectral measure for network robustness: the second spectral moment <i>m<sub>2</sub></i> of the network. Our results show that a smaller second spectral moment <i>m<sub>2</sub></i> indicates a more robust network. We demonstrate both theoretically and with extensive empirical studies that the second spectral moment can help
                        <ul>
                          <li> capture various traditional measures of network robustness;  </li>
                          <li> assess the robustness of networks;</li>
                          <li> design networks with controlled robustness; </li>
                          <li> study how complex networked systems (e.g., power systems) behave under cascading failures. </li>
                        </ul>
                      </p>
                    </div>
                </td>
            </tr>

            <!--publication-->
            <tr>
               <td width="32%">
                <img class="thumbnail" src="images/kdd2022.jpg" data-id="hbar-carousel2"></img>
                <div id="hbar-carousel2" class="carousel">
                  <img class="img-center-carousel" src="images/kdd2022.jpg" alt="">
                  <a href="javascript:void(0)"><img src="images/kdd2022.jpg" alt="" data-carousel-id="hbar-carousel2" class="carousel-close"></a>
                </div>
              </td>

              <td style="padding:15px;width:80%;vertical-align:middle">
                <a href="https://dl.acm.org/doi/pdf/10.1145/3534678.3539433?casa_token=PO2Pr9m4FV0AAAAA:90IEhLc3rmUGlgofQE-GM7kgNi1dY9V8igdsAdqjfrS7s1NRwS4kGUZbEEsGroQW7qPVzaKKQsJ4GQ">
                  <papertitle>A Spectral Representation of Networks: The Path of Subgraphs</papertitle>
                </a>
                <br>
                Shengmin Jin, Hao Tian, <b>Jiayu Li</b>, Reza Zafarani<br>
                <em>Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (<b>KDD</b>), 2022. <br> </em>
                [<a href="https://dl.acm.org/doi/pdf/10.1145/3534678.3539433?casa_token=PO2Pr9m4FV0AAAAA:90IEhLc3rmUGlgofQE-GM7kgNi1dY9V8igdsAdqjfrS7s1NRwS4kGUZbEEsGroQW7qPVzaKKQsJ4GQ">paper</a>]
                <!-- [<a href="">paper</a>] [<a href="">code</a>] -->
                <br>
                 <a href="javascript:toggle_vis('KDD')">More </a> 
                    <div id="KDD" style="display:none">
                      <p style="text-align:justify"> We propose a 3D network representation that relies on the spectral information of subgraphs: the Spectral Path, a path connecting the spectral moments of the network and those of its subgraphs of different sizes. We show that the spectral path is interpretable and can capture relationship between a network and its subgraphs, for which we present a theoretical foundation. We demonstrate the effectiveness of the spectral path in applications such as network visualization and network identification.
                      </p>
                    </div>
                </td>
            </tr>


            <!--publication-->
            <tr>
              <td width="32%">
                <img class="thumbnail" src="images/icassp2022.jpg" data-id="hbar-carousel3"></img>
                <div id="hbar-carousel3" class="carousel">
                  <img class="img-center-carousel" src="images/icassp2022.jpg" alt="">
                  <a href="javascript:void(0)"><img src="images/icassp2022.jpg" alt="" data-carousel-id="hbar-carousel3" class="carousel-close"></a>
                </div>
              </td>

              <td style="padding:15px;width:80%;vertical-align:middle">
                <a href="https://reza.zafarani.net/publications/files/AdverSparse.pdf">
                  <papertitle>AdverSparse: An Adversarial Attack Framework for Deep Spatial-Temporal Graph Neural Networks</papertitle>
                </a>
                <br><b>Jiayu Li</b>, Tianyun Zhang, Shengmin Jin, Makan Fardad, Reza Zafarani<br>
                <em>Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (<b>ICASSP</b>), 2022. <br> </em>
                [<a href="https://reza.zafarani.net/publications/files/AdverSparse.pdf">paper</a>]
                <!-- [<a href="">paper</a>] [<a href="">code</a>] -->
                <br>
                 <a href="javascript:toggle_vis('ICASSP')">More </a> 
                    <div id="ICASSP" style="display:none">
                      <p style="text-align:justify"> We propose a sparse adversarial attack framework <b>ADVERSPARSE</b> to illustrate that when only a few key connections are removed in such graphs, hidden spatial dependencies learned by such spatial-temporal models are significantly impacted, leading to various issues such as increasing prediction errors. We formulate the adversarial attack as an optimization problem and solve it by the Alternating Direction Method of Multipliers (ADMM). Experiments show that <b>ADVERSPARSE</b> can find and remove key connections in these graphs, leading to malfunctioning models, even in models capable of learning hidden spatial dependencies.
                      </p>
                    </div>
                </td>
            </tr>


            <!--publication-->
            <tr>
               <td width="32%">
                <img class="thumbnail" src="images/jdsa.jpg" data-id="hbar-carousel4"></img>
                <div id="hbar-carousel4" class="carousel">
                  <img class="img-center-carousel" src="images/jdsa.jpg" alt="">
                  <a href="javascript:void(0)"><img src="images/jdsa.jpg" alt="" data-carousel-id="hbar-carousel4" class="carousel-close"></a>
                </div>
              </td>

              <td style="padding:15px;width:80%;vertical-align:middle">
                <a href="https://link.springer.com/article/10.1007/s41060-021-00288-8">
                  <papertitle>Graph Sparsification with Graph Convolutional Networks</papertitle>
                </a>
                <br><b>Jiayu Li</b>, Tianyun Zhang, Hao Tian, Shengmin Jin, Makan Fardad, Reza Zafarani<br>
                <em>International Journal of Data Science and Analytics (<b>JDSA</b>), 2022. <br> </em>
                [<a href="https://link.springer.com/article/10.1007/s41060-021-00288-8">paper</a>]
                <br>
                 <a href="javascript:toggle_vis('JDSA')">More </a> 
                    <div id="JDSA" style="display:none">
                      <p style="text-align:justify"> We propose <b>Sparsified Graph Convolutional Network (SGCN)</b>, a neural network graph sparsifier that sparsifies a graph by pruning some edges. We formulate sparsification as an optimization problem and solve it by an Alternating Direction Method of Multipliers (ADMM). The experiment illustrates that <b>SGCN</b> can identify highly effective subgraphs for node classification in GCN compared to other sparsifiers such as Random Pruning, Spectral Sparsifier and DropEdge. We also show that sparsified graphs provided by SGCN can be inputs to GCN, which leads to better or comparable node classification performance with that of original graphs in GCN, DeepWalk, GraphSAGE, and GAT. We provide insights on why SGCN performs well by analyzing its performance from the view of a low-pass filter.
                      </p>
                    </div>
                </td>
            </tr>


            <!--publication-->
            <tr>
              <td width="32%">
                <img class="thumbnail" src="images/pakdd2020.jpg" data-id="hbar-carousel5"></img>
                <div id="hbar-carousel5" class="carousel">
                  <img class="img-center-carousel" src="images/pakdd2020.jpg" alt="">
                  <a href="javascript:void(0)"><img src="images/pakdd2020.jpg" alt="" data-carousel-id="hbar-carousel5" class="carousel-close"></a>
                </div>
              </td>

              <td style="padding:15px;width:80%;vertical-align:middle">
                <a href="https://link.springer.com/chapter/10.1007/978-3-030-47426-3_22">
                  <papertitle>SGCN: A Graph Sparsifier Based on Graph Convolutional Networks</papertitle>
                </a>
                <br><b>Jiayu Li</b>, Tianyun Zhang, Hao Tian, Shengmin Jin, Makan Fardad, Reza Zafarani<br>
                <em>Proceedings of the 24th The Pacific-Asia Conference on Knowledge Discovery and Data Mining (<b>PAKDD</b>), 2020. <br> </em>
                [<a href="https://link.springer.com/chapter/10.1007/978-3-030-47426-3_22">paper</a>]
                <!-- [<a href="">paper</a>] [<a href="">code</a>] -->
                <br>
                 <a href="javascript:toggle_vis('PAKDD')">More </a> 
                    <div id="PAKDD" style="display:none">
                      <p style="text-align:justify"> We propose <b>Sparsified Graph Convolutional Network (SGCN)</b>, a neural network graph sparsifier that sparsifies a graph by pruning some edges. We formulate sparsification as an optimization problem, which we solve by an Alternating Direction Method of Multipliers (ADMM)-based solution. We show that sparsified graphs provided by <b>SGCN</b> can be used as inputs to GCN, leading to better or comparable node classification performance with that of original graphs in GCN, DeepWalk, and GraphSAGE.
                      </p>
                    </div>
                </td>
            </tr>

                         <!--publication-->
            <tr>
              <td width="32%">
                <img class="thumbnail" src="images/asplos2019.jpg" data-id="hbar-carousel6"></img>
                <div id="hbar-carousel6" class="carousel">
                  <img class="img-center-carousel" src="images/asplos2019.jpg" alt="">
                  <a href="javascript:void(0)"><img src="images/asplos2019.jpg" alt="" data-carousel-id="hbar-carousel6" class="carousel-close"></a>
                </div>
              </td>

              <td style="padding:15px;width:80%;vertical-align:middle">
                <a href="https://dl.acm.org/doi/pdf/10.1145/3297858.3304076">
                  <papertitle>ADMM-NN: An Algorithm-Hardware Co-Design Framework of DNNs Using Alternating Direction Methods of Multipliers</papertitle>
                </a>
                <br>Ao Ren, Tianyun Zhang, Shaokai Ye, <b>Jiayu Li</b>, Wenyao Xu, Xuehai Qian, Xue Lin, Yanzhi Wang<br>
                <em>Proceedings of the 24th International Conference on Architectural Support for Programming Languages and Operating Systems (<b>ASPLOS</b>), 2019. <br> </em>
                [<a href="https://dl.acm.org/doi/pdf/10.1145/3297858.3304076">paper</a>]
                <br>
                 <a href="javascript:toggle_vis('ASPLOS')">More </a> 
                    <div id="ASPLOS" style="display:none">
                      <p style="text-align:justify"> We present <b>ADMM-NN</b>, the first algorithm-hardware co-optimization framework of DNNs using Alternating Direction Method of Multipliers (ADMM). The <b>ADMM-NN</b> is a systematic, joint framework of DNN weight pruning and quantization using ADMM. It can be understood as a smart regularization technique with regularization target dynamically updated in each ADMM iteration, resulting in higher performance in model compression than the state-of-the-art. We perform ADMM-based weight pruning and quantization considering the computation reduction and energy efficiency improvement.
                      Without accuracy loss, ADMM-NN achieves 85× and 24× pruning on LeNet-5 and AlexNet models, respectively, — significantly higher than the state-of-the-art. Combining weight pruning and quantization, we achieve 1,910× and 231× reductions in overall model size on these two benchmarks . Highly promising results are also observed on other representative DNNs such as VGGNet and ResNet-50.
                      </p>
                    </div>
                </td>
            </tr>


             <!--publication-->
            <tr>
              <td width="32%">
                <img class="thumbnail" src="images/aaai2019.jpg" data-id="hbar-carousel7"></img>
                <div id="hbar-carousel7" class="carousel">
                  <img class="img-center-carousel" src="images/aaai2019.jpg" alt="">
                  <a href="javascript:void(0)"><img src="images/aaai2019.jpg" alt="" data-carousel-id="hbar-carousel7" class="carousel-close"></a>
                </div>
              </td>

              <td style="padding:15px;width:80%;vertical-align:middle">
                <a href="https://ojs.aaai.org/index.php/AAAI/article/view/4475">
                  <papertitle>Universal Approximation Property and Equivalence of Stochastic Computing-Based Neural Networks and Binary Neural Networks</papertitle>
                </a>
                <br>Yanzhi Wang, Zheng Zhan, Liang Zhao, Jian Tang, Siyue Wang, <b>Jiayu Li</b>, Bo Yuan, Wujie Wen, Xue Lin<br>
                <em>Proceedings of the AAAI Conference on Artificial Intelligence (<b>AAAI</b>), 2019. <br> </em>
                [<a href="https://ojs.aaai.org/index.php/AAAI/article/view/4475">paper</a>]
                <br>
                 <a href="javascript:toggle_vis('AAAI')">More </a> 
                    <div id="AAAI" style="display:none">
                      <p style="text-align:justify">We prove that the ”ideal” SCNNs and BNNs satisfy the universal approximation property with probability 1 (due to the stochastic behavior). We also derive an appropriate bound for bit length M in order to provide insights for the actual neural network implementations. We further prove that SCNNs and BNNs exhibit the same energy complexity.
                      </p>
                    </div>
                </td>
            </tr>

  </table>


 <!-- here for reviews -->
 <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;margin-top:20px">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Conference/Journal Reviewer and Service</heading>
                  <p>
                     <ul>
                    <li> KDD 2019, KDD 2020, KDD 2021, KDD 2022 </li>
                    <li> WWW 2018, WWW 2019, WWW 2020 </li>
                    <li> SIGIR 2021, SIGIR 2022, SIGIR 2023 </li>
                    <li> WSDM 2018, WSDM 2019, WSDM 2020 </li>
                    <li> CIKM 2020, CIKM 2021, CIKM 2022 </li>

                    <li> <a href="javascript:toggle_vis('reviews')">More reviews</a> </li>
                    <div id="reviews" style="display:none">
                    <li> AAAI 2019 </li>
                    <li> ECML-PKDD 2020 </li>
                    <li> PAKDD 2018, PAKDD 2019, PAKDD 2020, PAKDD 2021 </li>
                    <li> International Journal of Neural Computing and Applications </li>
                    <li> International Journal of Computer Science and Technology </li>
                    <li> IEEE Access </li>
                    </div>
                  </ul>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

<!-- here for reward -->
 <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;margin-top: -10px">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Honors and Awards</heading>
                  <p>
                     <ul>
                    <li> ICASSP 2022 Student Travel Grant </li>
                    <li> PAKDD 2020 Student Travel Grant </li>
                    <li> Syracuse University Travel Grant 2018, 2020, 2022 </li>
                    <li> Syracuse University Graduate Grant, 2015 </li>
                  </ul>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

</body>
</html>
